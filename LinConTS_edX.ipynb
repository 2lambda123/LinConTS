{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thompson Sampling for Linearly Constrained Bandits \n",
    "## Finite-Time Regret and Violation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'font.size': 22, 'lines.linewidth' : 3})\n",
    "\n",
    "ray.init(redis_address=\"172.18.0.15:15672\", ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv( '../datasets/edX-Course.csv', thousands=',' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_rate = data['Participants']\n",
    "part_rate_normed =  ( part_rate - min( part_rate ) ) / ( max(part_rate) - min( part_rate ) )\n",
    "\n",
    "cert_rate = data['Certified'] / data['Participants']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( part_rate_normed * cert_rate )\n",
    "\n",
    "plt.xlabel('Arm Index')\n",
    "plt.ylabel('Expected Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def f():\n",
    "    time.sleep(0.01)\n",
    "    return ray.services.get_node_ip_address()\n",
    "\n",
    "# Get a list of the IP addresses of the nodes that have joined the cluster.\n",
    "ray_node_ips = set(ray.get([f.remote() for _ in range(1000)]))\n",
    "print('Number of active Ray nodes: %d'%(len(ray_node_ips)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_event_mean = part_rate_normed\n",
    "reward_value = cert_rate\n",
    "\n",
    "nrof_arms = len( reward_event_mean )\n",
    "\n",
    "constraint = 0.2\n",
    "\n",
    "T = 1000\n",
    "N = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self, success_prob):\n",
    "        self.success_prob = success_prob\n",
    "         \n",
    "    def pull(self, arm):\n",
    "        if np.random.rand() < self.success_prob[arm]:\n",
    "            return 1 # Success event\n",
    "        else:\n",
    "            return 0 # No reward event\n",
    "            \n",
    "    # Return the success probability for Oracle and for debugging purposes\n",
    "    def get_success_prob(self):\n",
    "        return self.success_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Constrained Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix, solvers\n",
    "\n",
    "# nrof_arms: Number of bandit arms (K)\n",
    "# reward_value: Reward value for each arm (r_k) if successful\n",
    "# target_success_prob: Target success probability\n",
    "# window_size: Window size for sliding window bandit. Events outside the window are discarded\n",
    "class BaseConstrainedBandit():\n",
    "    def __init__(self,\n",
    "                 nrof_arms,\n",
    "                 reward_value,\n",
    "                 target_success_prob=0.0):\n",
    "\n",
    "        self.nrof_arms = nrof_arms\n",
    "        self.reward_value = reward_value\n",
    "\n",
    "        self.target_success_prob = target_success_prob\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "        self.pull_count = [0 for _ in range(self.nrof_arms)]\n",
    "        self.success_count = [0 for _ in range(self.nrof_arms)]\n",
    "\n",
    "    # Determine which arm to be pulled\n",
    "    def act(self): # Implemented in child classes\n",
    "        pass\n",
    "\n",
    "    # Update the bandit\n",
    "    def update(self, arm, success):\n",
    "        self.t += 1\n",
    "\n",
    "        self.pull_count[arm] += 1\n",
    "        self.success_count[arm] += success\n",
    "\n",
    "    # Calculate the selection probability vector by solving a linear program\n",
    "    def calculate_selection_probabilities(self, success_prob, tolerance=1e-5):\n",
    "        c = matrix(-1 * np.array(success_prob) * np.array(self.reward_value))\n",
    "\n",
    "        neg_success_prob = [-1.0 * r for r in success_prob]\n",
    "\n",
    "        G = matrix(np.vstack([neg_success_prob, -1.0 * np.eye(self.nrof_arms)]))\n",
    "        h = matrix(np.append(-1 * self.target_success_prob, np.zeros((1, self.nrof_arms))))\n",
    "\n",
    "        A = matrix(np.ones((1, self.nrof_arms)))\n",
    "        b = matrix([1.0])\n",
    "\n",
    "        sol = solvers.lp(c, G, h, A, b)\n",
    "\n",
    "        selection_prob = np.reshape(np.array(sol['x']), -1)\n",
    "\n",
    "        if None in selection_prob: # Unsolvable optimiation\n",
    "            return [None]\n",
    "\n",
    "        # Fix numerical issues\n",
    "        selection_prob[np.abs(selection_prob) < tolerance] = 0.0  # Remove precision-related values\n",
    "        selection_prob = selection_prob / sum(selection_prob)     # Recalibrate probability vector to sum to 1\n",
    "\n",
    "        return selection_prob\n",
    "\n",
    "    # Sample from the probabilistic selection vector\n",
    "    def sample_prob_selection_vector(self, prob):\n",
    "        try:\n",
    "            return np.argwhere(np.random.multinomial(1, prob))[0][0]\n",
    "        except: # Throws ValueError somtimes\n",
    "            print('Error thrown by prob sampling. Returning random sample')\n",
    "            return np.random.randint(0, self.nrof_arms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oracle Constrained Bandit\n",
    "## Implements the stationary optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrof_arms: Number of bandit arms (K)\n",
    "# reward_value: Reward value for each arm (r_k) if successful\n",
    "# target_success_prob: Target success probability\n",
    "# success_prob: Success probability for each bandit arm\n",
    "class OracleConstrainedBandit(BaseConstrainedBandit):\n",
    "    def __init__(self,\n",
    "                 nrof_arms,\n",
    "                 reward_value,\n",
    "                 target_success_prob=0.0,\n",
    "                 env_instance=None):\n",
    "\n",
    "        super().__init__(nrof_arms, reward_value, target_success_prob)\n",
    "        self.env = env_instance\n",
    "\n",
    "        success_prob = self.env.get_success_prob()\n",
    "        self.selection_prob = self.calculate_selection_probabilities(success_prob)\n",
    "\n",
    "    # Determine which arm to be pulled\n",
    "    def act(self):\n",
    "        return self.sample_prob_selection_vector(self.selection_prob)\n",
    "\n",
    "    # Get selection probabilties (for debugging purposes)\n",
    "    def get_selection_prob(self):\n",
    "        return self.selection_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinCon-KL-UCB Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-15\n",
    "\n",
    "# Adopted from\n",
    "# https://nbviewer.jupyter.org/github/Naereen/notebooks/blob/master/Kullback-Leibler_divergences_in_native_Python__Cython_and_Numba.ipynb#Generic-KL-UCB-indexes,-with-a-bisection-search\n",
    "def klBern(x, y):\n",
    "    r\"\"\" Kullback-Leibler divergence for Bernoulli distributions. https://en.wikipedia.org/wiki/Bernoulli_distribution#Kullback.E2.80.93Leibler_divergence\n",
    "\n",
    "    .. math:: \\mathrm{KL}(\\mathcal{B}(x), \\mathcal{B}(y)) = x \\log(\\frac{x}{y}) + (1-x) \\log(\\frac{1-x}{1-y}).\"\"\"\n",
    "    x = min(max(x, eps), 1 - eps)\n",
    "    y = min(max(y, eps), 1 - eps)\n",
    "    return x * np.log(x / y) + (1 - x) * np.log((1 - x) / (1 - y))\n",
    "\n",
    "def estimate_kl_ucb(x, d, kl, upperbound, lowerbound=float('-inf'), precision=1e-6, max_iterations=50):\n",
    "    \"\"\" The generic KL-UCB index computation.\n",
    "\n",
    "    - x: value of the cum reward,\n",
    "    - d: upper bound on the divergence,\n",
    "    - kl: the KL divergence to be used (:func:`klBern`, :func:`klGauss`, etc),\n",
    "    - upperbound, lowerbound=float('-inf'): the known bound of the values x,\n",
    "    - precision=1e-6: the threshold from where to stop the research,\n",
    "    - max_iterations: max number of iterations of the loop (safer to bound it to reduce time complexity).\n",
    "\n",
    "    .. note:: It uses a **bisection search**, and one call to ``kl`` for each step of the bisection search.\n",
    "    \"\"\"\n",
    "    value = max(x, lowerbound)\n",
    "    u = upperbound\n",
    "    _count_iteration = 0\n",
    "    while _count_iteration < max_iterations and u - value > precision:\n",
    "        _count_iteration += 1\n",
    "        m = (value + u) / 2.\n",
    "        if kl(x, m) > d:\n",
    "            u = m\n",
    "        else:\n",
    "            value = m\n",
    "    return (value + u) / 2.\n",
    "\n",
    "# nrof_arms: Number of bandit arms (K)\n",
    "# reward_value: Reward value for each arm (r_k) if successful\n",
    "# target_success_prob: Target success probability\n",
    "class LinConKLUCBBandit(BaseConstrainedBandit):\n",
    "    def __init__(self,\n",
    "                 nrof_arms,\n",
    "                 reward_value,\n",
    "                 target_success_prob=0.0):\n",
    "\n",
    "        super().__init__(nrof_arms, reward_value, target_success_prob)\n",
    "\n",
    "    # Determine which arm to be pulled\n",
    "    def act(self):\n",
    "        # Ensure that all arms are pulled at least once\n",
    "        if self.t < self.nrof_arms:\n",
    "            return self.t\n",
    "\n",
    "        # Calculate the current KL-UCB for each arm\n",
    "        kl_ucb = [self.calculate_kl_ucb(arm) for arm in range(self.nrof_arms)]\n",
    "\n",
    "        # If not unimodal, select the arm constrained KL-UCB algorithm\n",
    "        kl_ucb_prob = self.calculate_selection_probabilities(kl_ucb)\n",
    "\n",
    "        if None in kl_ucb_prob: # Unsolvable optimization\n",
    "            return np.random.randint(0, self.nrof_arms)\n",
    "        else:\n",
    "            return self.sample_prob_selection_vector(kl_ucb_prob)\n",
    "\n",
    "    # Calculate KL-UCB for the specified arm\n",
    "    def calculate_kl_ucb(self, arm):\n",
    "        empirical_mean = self.success_count[arm] / self.pull_count[arm]\n",
    "\n",
    "        return estimate_kl_ucb(empirical_mean, np.log(self.t) / self.pull_count[arm], klBern, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinConTS Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrof_arms: Number of bandit arms (K)\n",
    "# reward_value: Reward value for each arm (r_k) if successful\n",
    "# target_success_prob: Target success probability\n",
    "class LinConTSBandit(BaseConstrainedBandit):\n",
    "    def __init__(self,\n",
    "                 nrof_arms,\n",
    "                 reward_value,\n",
    "                 target_success_prob=0.0):\n",
    "\n",
    "        super().__init__(nrof_arms, reward_value, target_success_prob)\n",
    "\n",
    "    # Determine which arm to be pulled\n",
    "    def act(self):\n",
    "        # Ensure that each arm is pulled at least once\n",
    "        if self.t < self.nrof_arms:\n",
    "            return self.t\n",
    "\n",
    "        # Sample a success probability from beta distribution Beta(a, b)\n",
    "        # where a = 1 + self.success_count[arm]\n",
    "        # and   b = 1 + self.pull_count[arm] - self.success_count[arm]\n",
    "        sampled_success_prob = [ np.random.beta(1 + self.success_count[arm],\n",
    "                                                1 + self.pull_count[arm] - self.success_count[arm])\n",
    "                                for arm in range(self.nrof_arms)]\n",
    "\n",
    "        # Success probability constraint through linear programming\n",
    "        ts_prob = self.calculate_selection_probabilities(sampled_success_prob)\n",
    "        if None in ts_prob: # Unsolvable optimization\n",
    "            return np.random.randint(0, self.nrof_arms)\n",
    "        else:\n",
    "            return self.sample_prob_selection_vector(ts_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to run experiment for a given policy and given time horizon\n",
    "@ray.remote # Comment out this line to run locally\n",
    "def run_experiment(policy, T, play_result, reward_value):\n",
    "    outcome = np.zeros((T, 2))\n",
    "    for t in range(T):    \n",
    "        arm = policy.act()\n",
    "        \n",
    "        success = play_result[t][arm]\n",
    "\n",
    "        outcome[t, 0] = success\n",
    "        \n",
    "        if success:\n",
    "            outcome[t, 1] = reward_value[arm]\n",
    "        \n",
    "        policy.update(arm, success)\n",
    "        \n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the policy\n",
    "def play(policy, T, N, play_result, reward_value, name):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Uncomment the following line to run locally\n",
    "    #outcome = [run_experiment(policy, T, play_result[n], reward_value) for n in range(N)]\n",
    "    outcome = ray.get([run_experiment.remote(policy, T, play_result[n], reward_value) for n in range(N)])\n",
    "        \n",
    "    total_success = np.zeros((N, T))\n",
    "    total_reward = np.zeros((N, T))\n",
    "    for n in range(N):\n",
    "        total_success[n, :] = outcome[n][:, 0]\n",
    "        total_reward[n, :] = outcome[n][:, 1]\n",
    "    \n",
    "    print(name + ' done! Elasped: %0.2fs'%(time.time() - start))\n",
    "    \n",
    "    return ( total_success, total_reward )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all events in advance\n",
    "np.random.seed(42)\n",
    "env = Environment( reward_event_mean )\n",
    "\n",
    "play_result = [[[env.pull(arm) for arm in range(nrof_arms)] for _ in range(T)] for _ in range(N)]\n",
    "\n",
    "# Suppress LP solver output\n",
    "solvers.options['show_progress'] = False\n",
    "solvers.options['glpk']          = {'msg_lev': 'GLP_MSG_OFF'}  # cvxopt 1.1.8\n",
    "solvers.options['msg_lev']       = 'GLP_MSG_OFF'               # cvxopt 1.1.7\n",
    "solvers.options['LPX_K_MSGLEV']  = 0                           # previous versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create the policy objects\n",
    "OraclePolicy = OracleConstrainedBandit( nrof_arms, reward_value, constraint, env_instance=env )\n",
    "\n",
    "LinConTSPolicy = LinConTSBandit( nrof_arms, reward_value, constraint )\n",
    "\n",
    "LinConKLUCBPolicy = LinConKLUCBBandit( nrof_arms, reward_value, constraint )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the policies and collect results\n",
    "oracle_reward_events, oracle_reward_values = play( OraclePolicy, \n",
    "                                                   T, \n",
    "                                                   N, \n",
    "                                                   play_result, \n",
    "                                                   reward_value, \n",
    "                                                   'Oracle' )\n",
    "\n",
    "lincon_ts_reward_events, lincon_ts_reward_values = play( LinConTSPolicy, \n",
    "                                                         T, \n",
    "                                                         N, \n",
    "                                                         play_result, \n",
    "                                                         reward_value,\n",
    "                                                         'Con-TS' )\n",
    "\n",
    "lincon_kl_ucb_reward_events, lincon_kl_ucb_reward_values = play( LinConKLUCBPolicy, \n",
    "                                                                 T, \n",
    "                                                                 N, \n",
    "                                                                 play_result, \n",
    "                                                                 reward_value,\n",
    "                                                                 'Con-KL-UCB' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "data = {'T': T,\n",
    "        'N': N,\n",
    "        'oracle_reward_events'        : oracle_reward_events,\n",
    "        'oracle_reward_values'        : oracle_reward_values,\n",
    "        'lincon_kl_ucb_reward_events' : lincon_kl_ucb_reward_events,\n",
    "        'lincon_kl_ucb_reward_values' : lincon_kl_ucb_reward_values,\n",
    "        'lincon_ts_reward_events'     : lincon_ts_reward_events,\n",
    "        'lincon_ts_reward_values'     : lincon_ts_reward_values,\n",
    "        'constraint'                  : constraint}\n",
    "\n",
    "filename = 'RESULTS_edX_T%d_N%d.npy'%( T, N )\n",
    "np.save(filename, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
